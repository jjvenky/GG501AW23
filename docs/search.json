[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GG501",
    "section": "",
    "text": "1 Introduction\nScheduled as Wednesdays 08:30–11:20, P331 (Peters building)"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "GG501",
    "section": "1.1 Course Description",
    "text": "1.1 Course Description\nTranslating data into actionable insights requires more than analytical tools; the ability to communicate, visualize, deconstruct and interrogate analytic results is an increasingly required set of skills for environmental professionals. This course builds the skills and knowledge needed to interpret, critique, and communicate outputs from data-analytic workflows and processes. Key issues discussed include parameterization, sensitivity analysis, visualization, and summarizing results for a variety of audiences."
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "GG501",
    "section": "1.2 Learning Objectives",
    "text": "1.2 Learning Objectives\n\ncreate and critique visualizations for disparate data types\ncritically evaluate statistical and machine learning model output\ncritically evaluate the data quality dimensions of environmental data workflows\ndescribe approaches for incorporating data science outputs into environmental decision making"
  },
  {
    "objectID": "index.html#goals-of-the-course",
    "href": "index.html#goals-of-the-course",
    "title": "GG501",
    "section": "1.3 Goals of the Course",
    "text": "1.3 Goals of the Course\nThis course will develop skills that allow students to ask critical questions of environmental data analytic tools. We will focus on visualization and model assessment techniques. Students will be expected to work with statistical programming language R in the course."
  },
  {
    "objectID": "index.html#required-text",
    "href": "index.html#required-text",
    "title": "GG501",
    "section": "1.4 Required Text",
    "text": "1.4 Required Text\n\nWickham H, Navaro D, Pedersen TL. 2021. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. Springer. https://ggplot2-book.org\nChang W. 2021. R Graphics Cookbook, 2nd Edition. https://r-graphics.org"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "GG501",
    "section": "1.5 Schedule",
    "text": "1.5 Schedule\n\n\n\n\n\n\n\n\nWeek\nTopic\nTools\n\n\n\n\nJanuary 11\nIntroduction, spatial knowledge mobilization, key concepts\n\n\n\nJanuary 18\nTabular visualization\nR base , ggplot2\n\n\nJanuary 25\nTime series visualization\nR ggplot2\n\n\nFebruary 1\nSpatial visualization\nRggplot2, tmap, mapsf\n\n\nFebruary 8\nSpatial visualization\nRggplot2, tmap, mapsf\n\n\nFebruary 15\nModel visualization\nRggplot2, broom, jtools\n\n\nFebruary 22\nReading Week\n\n\n\nMarch 1\nWork Period & Catch Up!\n\n\n\nMarch 8\nModel parameterization and validation\nR stat, lme4\n\n\nMarch 15\nModel parameterization and validation\nRrandomForest, kmeans, spgwr\n\n\nMarch 22\nModelling: Socio-Technical Critique\nReadings\n\n\nMarch 29\nCommunication strategies for EDA\nRmarkdown, Shiny, Quarto?\n\n\nApril 5\nTeam Project Presentations"
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "GG501",
    "section": "1.6 Evaluation",
    "text": "1.6 Evaluation\n\n\n\n\n\n\n\n\nAssessment\nWeighting\nDue Date\n\n\n\n\nAssignments (3x15%)\n45%\nFebruary 1 March 8 March 22\n\n\nAnalytics Demo\n40%\nMarch 21 & 28\n\n\nParticipation\n15%\n\n\n\nTotal\n100%"
  },
  {
    "objectID": "index.html#evaluation-items",
    "href": "index.html#evaluation-items",
    "title": "GG501",
    "section": "1.7 Evaluation items",
    "text": "1.7 Evaluation items\n\n1.7.1 Assignments\n\nThree assignments will require students to demonstrate data visualization and critique skills learned in the course. Students will be able to use their own datasets or any publicly available datasets of their choosing.\n\n\n\n1.7.2 Term Project\n\nStudents will work on a major project which will require either an independent data analysis project, or a detailed critique/reproduction of an existing published analysis.\n\n\n\n1.7.3 Participation\n\nStudents will be expected to attend and participate in designated course times. Participate includes contributing to discussions and working collaboratively with other students when needed."
  },
  {
    "objectID": "dataanalytic-visualization.html",
    "href": "dataanalytic-visualization.html",
    "title": "2  Data/Analytic Visualization",
    "section": "",
    "text": "3 Data/Analytic Visualization"
  },
  {
    "objectID": "dataanalytic-visualization.html#readings-this-week",
    "href": "dataanalytic-visualization.html#readings-this-week",
    "title": "2  Data/Analytic Visualization",
    "section": "3.1 Readings this week",
    "text": "3.1 Readings this week\n\nTCL-4 Effective Visualization\nHMP-1 Introduction\nHNP-2 First Steps"
  },
  {
    "objectID": "dataanalytic-visualization.html#readings-this-week-1",
    "href": "dataanalytic-visualization.html#readings-this-week-1",
    "title": "2  Data/Analytic Visualization",
    "section": "4.1 Readings this week",
    "text": "4.1 Readings this week\nHNP-3 Individual geoms HNP-4 Collective geoms HNP-5 Statistical summaries ggplot2 Cheat Sheet"
  },
  {
    "objectID": "dataanalytic-visualization.html#case-study",
    "href": "dataanalytic-visualization.html#case-study",
    "title": "2  Data/Analytic Visualization",
    "section": "4.2 Case study",
    "text": "4.2 Case study\nSurveying farmers about agricultural practices. This project was in consultation with an environmental agriculture association aiming to understand barriers to the adoption of specific farming practices, specifically the use of cover crops.\n\n\n\nRed Clover cover crops on farm field. Image credit: https://www.country-guide.ca/\n\n\nThe survey asked farmers about their use of cover crops and any barriers they experienced in applying this practice to their farms. We will walk-through a detailed analysis of this farmer survey data, which is primarily tabular data with some spatial and temporal components.\nFirst, let’s look at the raw data.\n\n\n\nSnapshot of farmer survey data\n\n\nSome things you need to check when inspecting a new dataset, include at least the following:\n\nare the column names well-formed\nwhat columns have NULL values, will this cause an issue ** how are NULL values coded, is it consistent?\ndo any columns have a mix of data types? (e.g., see column AH)\nis there data that should anonymized?\nare there gaps above or below the data, or ad hoc summaries that need to be removed?\nare there any obviously erroneous values (i.e., not valid outliers but errors, such as a value of 491 in the age column) that need to be removed\n\nAs well as some deeper questions:\n\ndo you understand what each column and value means, what each row represents? If there are some you do not understand consider whether the columns you do understand are sufficient to answer the key questions you're exploring with this dataset\nare the data stored in the correct data type (especially important if viewing data in Microsoft Excel)\nare the data complete? (i.e., are all the expected rows there, does the rowcount = the sample size)\n\nSecondly, let's examine the end product we created to explore this data. The graphs here were used in a report summarizign key trends."
  },
  {
    "objectID": "dataanalytic-visualization.html#skm-introduction",
    "href": "dataanalytic-visualization.html#skm-introduction",
    "title": "2  Data/Analytic Visualization",
    "section": "2.1 SKM Introduction",
    "text": "2.1 SKM Introduction\n\n2.1.1 Readings this week\n\nTCL-4 Effective Visualization\nHMP-1 Introduction\nHNP-2 First Steps"
  },
  {
    "objectID": "dataanalytic-visualization.html#tabular-data",
    "href": "dataanalytic-visualization.html#tabular-data",
    "title": "2  Data/Analytic Visualization",
    "section": "2.2 Tabular Data",
    "text": "2.2 Tabular Data\n\n2.2.1 Readings this week\nHNP-3 Individual geoms HNP-4 Collective geoms HNP-5 Statistical summaries ggplot2 Cheat Sheet\n\n\n2.2.2 Case study\nSurveying farmers about agricultural practices. This project was in consultation with an environmental agriculture association aiming to understand barriers to the adoption of specific farming practices, specifically the use of cover crops.\n\n\n\nRed Clover cover crops on farm field. Image credit: https://www.country-guide.ca/\n\n\nThe survey asked farmers about their use of cover crops and any barriers they experienced in applying this practice to their farms. We will walk-through a detailed analysis of this farmer survey data, which is primarily tabular data with some spatial and temporal components.\nFirst, let’s look at the raw data.\n\n\n\nSnapshot of farmer survey data\n\n\nSome things you need to check when inspecting a new dataset, include at least the following:\n\nare the column names well-formed?\nwhat columns have NULL values, will this cause an issue, how are NULL values coded, is it consistent?\ndo any columns have a mix of data types? (e.g., see column AH)\nare there data that should anonymized?\nare there gaps above or below the data, or ad hoc summaries that need to be removed?\nare there any obviously erroneous values (i.e., not valid outliers but errors, such as a value of 491 in the age column) that need to be removed?\n\nAs well as some deeper questions:\n\ndo you understand what each column and value means, what each row represents? If there are some you do not understand consider whether the columns you do understand are sufficient to answer the key questions you’re exploring with this dataset?\nare the data stored in the correct data type (especially important if viewing data in Microsoft Excel)?\nare the data complete? (i.e., are all the expected rows there, does the rowcount = the sample size)?\n\nSecondly, let’s examine the end product we created to explore this data. The graphs here were used in a report summarizing key trends.\n\n\n\nScreenshot of farmer survey report. View the full report here."
  },
  {
    "objectID": "dataanalytic-visualization.html#time-series-data",
    "href": "dataanalytic-visualization.html#time-series-data",
    "title": "2  Data/Analytic Visualization",
    "section": "2.3 Time Series Data",
    "text": "2.3 Time Series Data\n\n2.3.1 Readings this week\n\nNeon Tutorial - Time Series with ggplot2 in R\nA Little Book of R for Time Series\nLubridate Cheat Sheet"
  },
  {
    "objectID": "dataanalytic-visualization.html#spatial-data",
    "href": "dataanalytic-visualization.html#spatial-data",
    "title": "2  Data/Analytic Visualization",
    "section": "2.4 Spatial Data",
    "text": "2.4 Spatial Data\n\n2.4.1 Readings this week\n\nLovelace-2 Geographic Data in R\nHNP-6 Maps\nPebezma - 2018\nsimple features webpage\nSpatial Objects in R Tutorial\nSpatial analysis in R Demo\n\n\n\n2.4.2 Case Study\nHere we will walk through a spatial visualization analysis from start to finish and make some key decisions about how we want to visualize the data. The data we are going to start with is from crime statistics in the City of Toronto. We can find the materials for this walk-through here."
  },
  {
    "objectID": "dataanalytic-visualization.html#model-data",
    "href": "dataanalytic-visualization.html#model-data",
    "title": "2  Data/Analytic Visualization",
    "section": "2.5 Model Data",
    "text": "2.5 Model Data\nStatisical modelling is a core construct in R due to its roots as a statistical programming language. Base R has powerful modelling features such as lm and glm for fitting models to data. As well, there are hundreds (thousands?) of additional packages that implement almost any conceivable statistical model, from time series to spatial point process models to survival models. This week we will review some basic ideas of randomness and modelling and then explore how to visualize and interrogate models using tidyverse friendly packages and tools.\n\n2.5.1 Resources this week\n\nKabakoff - Ch. 8 statistical models\nbootstrapping and visualizing models\nbroom and dplyr vignette\njtools package\nvisualizing models 101"
  },
  {
    "objectID": "dataanalytic-critique.html#spatial-modelling-example---adapted-workshop-example",
    "href": "dataanalytic-critique.html#spatial-modelling-example---adapted-workshop-example",
    "title": "3  Data/Analytic Critique",
    "section": "3.1 Spatial modelling example - adapted workshop example",
    "text": "3.1 Spatial modelling example - adapted workshop example\nThe data for this workshop demo exists as multiple files and folders, so we have zipped them up and posted them to this github repo. Note that we have to use the raw link to get access to the actual data file not the page that shows the file, with that link we can follow the workflow we have used before to download, unzip and remove the zip file. Note that his zip file is large (~17 mb) so we probably only want to download it once.\n#download.file(\"https://github.com/colinr23/gg501/blob/main/data/workshop_data.zip?raw=true\", \n#              destfile = \"workshop_data.zip\" , mode='wb')\n#unzip(\"workshop_data.zip\", exdir = \".\")\n#file.remove(\"workshop_data.zip\")\nRead in and examine the data:\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tibble)\n\noptions(scipen=6)\nplt <- read_csv(\"workshop_data/SampleData_Basics/plt.csv\", \n                col_types = list(col_character(), col_integer(), \n                col_integer(), col_integer(), col_integer(), col_double(), \n                col_double(), col_integer(), col_double(), col_double(), \n                col_character(), col_integer()))\ntree <- read_csv(\"workshop_data/SampleData_Basics/tree.csv\", col_types = list(col_character()))\nref <- read_csv(\"workshop_data/SampleData_Basics/ref_SPCD.csv\")\n\ntree <- left_join(tree, ref, by=\"SPCD\")\n\nspfreq <- tree %>% select(PLT_CN, SPNM)  %>% table() %>% as.data.frame.matrix()  %>% select(aspen) %>% rownames_to_column(\"PLT_CN\")\n#reset any counts greater than 1 to 1 --binary response\nspfreq$aspen[spfreq$aspen>0] <- 1\nplt2 <- left_join(plt, spfreq, by=c(\"CN\" = \"PLT_CN\"))\nNow we have created the plt2 dataset at the plot level which contains plots where aspen is present or absent. We will keep working with the instructions from the workshop slides and modernize some of the workflow:\n\n\n\nSlide from workshop.\n\n\n## Forest Inventory data (Model response)\n## Now, let's compile total carbon by plot and append to plot table.\n# First, create a table of counts by plot.\nplt2 <- tree %>% group_by(PLT_CN) %>% \n  summarise(CARBON_AG = sum(CARBON_AG)) %>% \n  mutate(CARBON_KG = round(CARBON_AG * 0.453592)) %>% \n  right_join(plt2, by = c(\"PLT_CN\" = \"CN\")) %>% \n  mutate(CARBON_KG=replace(CARBON_KG, is.na(CARBON_KG), 0)) %>% \n  mutate(aspen=replace(aspen, is.na(aspen), 0)) %>% \n  mutate(CARBON_AG = NULL)\nNow we can carry on with some of the spatial data processing to build the variables for modelling, a very common workflow:\n# Load libraries\nlibrary(rgdal)  # GDAL operations for spatial data\nlibrary(raster) # Analyzing gridded spatial data\nlibrary(rpart)  # Recursive partitioning and regression trees\nlibrary(car)    # For book (An R Companion to Applied Regression)\nlibrary(randomForest)   # Generates Random Forest models\nlibrary(PresenceAbsence)    # Evaluates results of presence-absence models\nlibrary(ModelMap)   # Generates and applies Random Forest models\n\n# We need to extract data from spatial layers, so let's convert the plot table to a SpatialPoints object in R.\n## We know the projection information, so we can add it to the SpatialPoints object. \nprj4str <- \"+proj=longlat +ellps=GRS80 +datum=NAD83 +no_defs\"\nptshp <- SpatialPointsDataFrame(plt[,c(\"LON\",\"LAT\")], plt,      proj4string = CRS(prj4str))\n## Display the points\nplot(ptshp)\nimage\n## Predictor variables:\n    # Landsat Thematic Mapper, band 5   30-m spectral data, band 5, resampled to 90 m\n    # Landsat Thematic Mapper, NDVI 30-m spectral data, NDVI, resampled to 90 m\n    # Classified forest/nonforest map       250-m classified MODIS, resampled to 90 m\n    # Elevation             30-m DEM, resampled to 90 m\n    # Slope                 90-m DEM – derived in a following slides\n  # Aspect  \n## Set file names\nb5fn <- \"workshop_data/SampleData_Spatial/uintaN_TMB5.img\"      # Landsat TM–Band5\nndvifn <- \"workshop_data/SampleData_Spatial/uintaN_TMndvi.img\"      # Landsat TM–NDVI\nfnffn <- \"workshop_data/SampleData_Spatial/uintaN_fnfrcl.img\"       # Forest type map (reclassed)\nelevfn <- \"workshop_data/SampleData_Spatial/uintaN_elevm.img\"       # Elevation (meters)\n\n## Reclass raster layer to 2 categories\nfnf <- raster(\"workshop_data/SampleData_Spatial/uintaN_fnf.img\")\n\n## Create raster look-up table\nfromvect <- c(0,1,2,3)\ntovect <- c(2,1,2,2)\nrclmat <- matrix(c(fromvect, tovect), 4, 2)\n## Generate raster and save to SpatialData folder\nfnfrcl <- reclassify(x=fnf, rclmat, datatype='INT2U', filename=\"workshop_data/SampleData_Spatial/uintaN_fnfrcl.img\", overwrite=TRUE)\n\n## Check rasters\nrastfnlst <- c(b5fn, ndvifn, fnffn, elevfn)\nrastfnlst\n\nsapply(rastfnlst, raster)\n\n## Now, let's generate slope from DEM. Save it to your workshop_data/SampleData_Spatial folder.\nhelp(terrain)\nhelp(writeRaster)\n\nslpfn <- \"workshop_data/SampleData_Spatial/uintaN_slp.img\" \nslope <- terrain(raster(elevfn), opt=c('slope'), unit='degrees',    filename=slpfn, datatype='INT1U', overwrite=TRUE)\n####************* this may take some time ****************  \nplot(slope, col=topo.colors(6))\nimage\n####************* this may take some time ****************\n\n\n## Check rasters\nrastfnlst <- c(b5fn, ndvifn, fnffn, elevfn, slpfn)\nrastfnlst\n\nsapply(rastfnlst, raster)\n\n## We can also generate aspect from DEM. Save it to your workshop_data/SampleData_Spatial folder.\nhelp(terrain)\n\n## This is an intermediate step, so we are not going to save it.\naspectr <- terrain(raster(elevfn), opt=c('aspect'), unit='radians')\naspectr\n\n# Note: Make sure to use radians, not degrees\n####************* this may take some time ****************\nplot(aspectr, col=terrain.colors(6))\nimage\n## Aspect is a circular variable. There are a couple ways to deal with this:\n\n## 1. Convert the values to a categorical variable (ex. North, South, West, East)\n\n## We derived aspect in radians. First convert radians to degrees.\naspectd <- round(aspectr * 180/pi)\naspectd\n\n## Now, create a look-up table of reclass values.\nhelp(reclassify)\nfrommat <- matrix(c(0,45, 45,135, 135,225, 225,315, 315,361), 5, 2)\nfrommat\n\nfrommat <- matrix(c(0,45, 45,135, 135,225, 225,315, 315,361), 5, 2, byrow=TRUE)\nfrommat\n\ntovect <- c(1, 2, 3, 4, 1)\n\nrclmat <- cbind(frommat, tovect)\nrclmat\n\n## Reclassify raster to new values.\naspcl <- reclassify(x=aspectd, rclmat, include.lowest=TRUE)\naspcl\n\nunique(aspcl)\n\nbks <- c(0,sort(unique(aspcl))) # Break points\ncols <- c(\"dark green\", \"wheat\", \"yellow\", \"blue\")  # Colors\nlabs <- c(\"North\", \"East\", \"South\", \"West\") # Labels\nlab.pts <- bks[-1]-diff(bks)/2  # Label position\n####************* this may take some time ****************\nplot(aspcl, col=cols, axis.args=list(at=lab.pts, labels=labs), breaks=bks)\nimages\n## 2. Convert to a linear variable (ex. solar radiation index; Roberts and Cooper 1989)\n\naspval <- (1 + cos(aspectr+30))/2   ## Roberts and Cooper 1989\naspval\n\nplot(aspval)\nimage\n## Let's multiply by 100 and round so it will be an integer (less memory)\naspval <- round(aspval * 100)\naspval\n\nplot(aspval)\nimage\n# Save this layer to file\naspvalfn <- \"workshop_data/SampleData_Spatial/uintaN_aspval.img\"\nwriteRaster(aspval, filename=aspvalfn, datatype='INT1U', overwrite=TRUE)\n\n# Add aspval to rastfnlst\nrastfnlst <- c(rastfnlst, aspvalfn)\n\n## Converts aspect into solar radiation equivalents, with a correction of 30 degrees to reflect ## the relative heat of the atmosphere at the time the peak radiation is received. \n## Max value is 1.0, occurring at 30 degrees aspect; min value is 0, at 210 degrees aspect. \n\n#Roberts, D.W., and S. V. Cooper. 1989. Concepts and techniques in vegetation mapping. In Land classifications based on vegetation: applications for resource management. D. Ferguson, P. Morgan, and F. D. Johnson, editors. USDA Forest Service General Technical Report INT-257, Ogden, Utah, USA.\nPredictor Data Extraction\n\n\nOne of the most common uses of GIS and spatial processing functions in r is to build spatial variables which are then used for modelling. Here we have some data at specific coordinates where field plots were located, and we have several raster layers where we have environmental covariates. We want to extract values of raster layers at the locations of the point data.\n## We need to check the projections of the rasters. If the projections are different,\n## reproject the points to the projection of the rasters, it is much faster.\n\n## We will use the plt2 table with LON/LAT coordinates and the response data attached.\nhead(plt2)\n\n\n## We know the LON/LAT coordinates have the following projection:\nprj4str <- \"+proj=longlat +ellps=GRS80 +datum=NAD83 +no_defs\"\n\n# Check projections of each raster..  \nsapply(rastfnlst, function(x){ projection(raster(x)) })\n\n## Reproject SpatialPoints object to match raster projections. \nhelp(project)\nrast.prj <- projection(raster(rastfnlst[1]))\nxy <- cbind(plt$LON, plt$LAT)\nxyprj <- project(xy, proj=rast.prj)\n## Extract values (raster package)\nhelp(extract)\n\n# Let's extract values from 1 layer.\ntmp <- extract(raster(elevfn), xyprj)\nhead(tmp)\n\n# Now, let's create a function to extract, so we can extract from all the rasters at the same time. \nextract.fun <- function(rast, xy){ extract(raster(rast), xy) }\n\n# Now, apply this function to the vector list of raster file names.\nrastext <- sapply(rastfnlst, extract.fun, xyprj)\n\n# Look at the output and check the class. \nhead(rastext)\n\nclass(rastext)\n\n## Extract values (raster package) cont..  change names\n\n# Let's make the column names shorter. \ncolnames(rastext)\n\n# Use the rastfnlst vector of file names to get new column names.\n# First, get the base name of each raster, without the extension.\ncnames <- unlist(strsplit(basename(rastfnlst), \".img\"))\ncnames\n\n# We could stop here, but let's make the names even shorter and remove\n# 'uintaN_' from each name.\ncnames2 <- substr(cnames, 8, nchar(cnames))\ncnames2\n\n# Now, add names to matrix. Because the output is a matrix, we will use colnames.\ncolnames(rastext) <- cnames2\nhead(rastext)\n\n# Now, let's append this matrix to the plot table with the response data (plt2).\nhead(plt2)\n\n# We just want the response variables, so let's extract these columns along with the unique identifier of the table (CN, aspen, CARBON_KG).\nmodeldat <- cbind(plt2[,c(\"PLT_CN\", \"aspen\", \"CARBON_KG\")], rastext)\nhead(modeldat)\n\n# Check if this is a data frame\nis.data.frame(modeldat)     \n\ndim(modeldat)\n\n# Let's also append the projected xy coordinates for overlaying with raster layers.\nmodeldat <- cbind(xyprj, modeldat)\nhead(modeldat)\n\ncolnames(modeldat)[1:2] <- c(\"X\", \"Y\")\nhead(modeldat)\nNow follow along with the Data Exploration section of the workshop slides. Think about what you would change and what you might leave the same regarding coding style and workflow.\nNow we will continue on with the Modelling Generation section of the workshop slides, following the code below.\n\nlibrary(rpart)\n\n## Classification tree\nasp.tree <- rpart(aspen ~ TMB5 + TMndvi + fnfrcl + elevm + slp + aspval,    data=modeldat, method=\"class\")\n\nplot(asp.tree)\ntext(asp.tree, cex=0.75)\n\n## Regression tree\ncarb.tree <- rpart(CARBON_KG ~ TMB5 + TMndvi + fnfrcl + elevm + slp + aspval,   data=modeldat)\nplot(carb.tree)\ntext(carb.tree, cex=0.75)\n\n## Now, let's use the randomForests package – Classification tree\nlibrary(randomForest)\nhelp(randomForest)\n\n## Let's try with ASPEN binary, categorical response (presence/absence)\nset.seed(66)\nasp.mod <- randomForest(factor(aspen) ~ TMB5 + TMndvi + fnfrcl + elevm + slp + aspval,  data=modeldat, importance = TRUE, na.action = na.exclude)\n\n\n## Default parameters:\n# ntree = 500   # Number of trees\n# mtry = sqrt(p)    # Number of predictors (p) randomly sampled at each node\n# nodesize = 1  # Minimum size of terminal nodes\n# replace = TRUE    # Bootstrap samples are selected with replacement   \n\n\n## Look at results\nasp.mod\n\nsummary(asp.mod)\n\nerr <- asp.mod$err.rate # Out-Of-Bag (OOB) error rate (of i-th element)\nhead(err)\n\ntail(err)\n\nmat <- asp.mod$confusion    # Confusion matrix\nmat\n\n## Classification tree - Output\n\n# Plot the number of trees by the error rate\nplot(1:500, err[,\"OOB\"], xlab=\"Number of trees\", ylab=\"Error rate\")\n\n# Note: how many trees needed to stabilize prediction\n\n## Calculate the percent correctly classified from confusion (error) matrix\nmat\n\npcc <- sum(diag(mat[,1:2]))/sum(mat) * 100\npcc\n\npcc <- round(pcc, 2)    ## Round to nearest 2 decimals\npcc\n\nlibrary(PresenceAbsence)\npcc(mat[,1:2], st.dev=TRUE)\n\nKappa(mat[,1:2], st.dev=TRUE)\n\n## The Kappa statistic summarizes all the available information in the confusion matrix.\n## Kappa measures the proportion of correctly classified units after accounting for the probability of chance agreement.\n\n## Now, let's use the randomForests package – regression tree\n\n## Now, let's try with the continuous, CARBON_KG response\nset.seed(66)\ncarb.mod <- randomForest(CARBON_KG ~ TMB5 + TMndvi + elevm + slp + aspval,  data=modeldat, importance = TRUE, na.action=na.exclude)\n\n## Default parameters:\n# ntree = 500   # Number of trees\n# mtry = p/3    # Number of predictors (p) randomly sampled at each node\n# nodesize = 5  # Minimum size of terminal nodes\n# replace = TRUE    # Bootstrap samples are selected with replacement   \n\n## Look at results\ncarb.mod\n\nsummary(carb.mod)\n\nnames(carb.mod)\n\n## Regression tree - Output\n\nnames(carb.mod)\n\nmse <- carb.mod$mse # Mean square error (of i-th element) \nrsq <- carb.mod$rsq # Pseudo R-squared (1-mse/Var(y))(of i-th element)\n\nhead(mse)\n\ntail(mse)\n\ntail(rsq)\n\n## Regression tree - Output\n\n# Plot the number of trees by the mse (Mean Square Error)\nplot(1:500, mse, xlab=\"Number of trees\", ylab=\"Mean Square Error rate\")\n\n# Note: how many trees needed to stabilize prediction\n\n# Similarly, plot the number of trees by the rsq (R-Squared)\nplot(1:500, rsq, xlab=\"Number of trees\", ylab=\"R-Squared\")\n\n# Again: how many trees needed to stabilize prediction\n\n## Variable importance – Classification tree\n\n## Get importance table\nasp.imp <- abs(asp.mod$importance)\nasp.imp\n\n## Get the number of measures (columns) and number of predictors\nncols <- ncol(asp.imp)      ## Number of measures\nnumpred <- nrow(asp.imp)        ## Get number of predictors\n\n\n## Plot the measures of variable importance for ASPEN presence/absence\npar(mfrow=c(2,2))\nfor(i in 1:ncols){  ## Loop thru the different importance measures\n    ivect <- sort(asp.imp[,i], dec=TRUE)        ## Get 1st measure, descending order\n    iname <- colnames(asp.imp)[i]       ## Get name of measure\n\n    # Generate histogram plot (type='h') with no x axis (xaxt='n')\n    plot(ivect, type = \"h\", main = paste(\"Measure\", iname), xaxt=\"n\",\n        xlab = \"Predictors\", ylab = \"\", ylim=c(0,max(ivect)))\n\n    # Add x axis with associated labels\n    axis(1, at=1:numpred, lab=names(ivect)) \n}\n\n## Let’s make a function and plot importance values for CARBON_KG model.\n\nplotimp <- function(itab){\n    ncols <- ncol(itab)         ## Number of measures\n    numpred <- nrow(itab)       ## Get number of predictors\n\n    ## Plot the measures of variable importance \n    par(mfrow = c(ncols/2,2))\n    for(i in 1:ncols){  ## Loop thru the different importance measures\n        ivect <- sort(itab[,i], dec=TRUE)   ## Get 1st measure, sorted decreasing\n        iname <- colnames(itab)[i]      ## Get name of measure\n\n        # Generate histogram plot (type='h') with no x axis (xaxt='n')\n        plot(ivect, type = \"h\", main = paste(\"Measure\", iname), xaxt=\"n\",\n            xlab = \"Predictors\", ylab = \"\", ylim=c(0,max(ivect)))\n\n        # Add x axis with associated labels\n        axis(1, at=1:numpred, lab=names(ivect)) }\n}\n\n## Check function with ASPEN model\nplotimp(asp.imp)\n\n## Now, run funtion with CARBON_KG model\nplotimp(carb.mod$importance)\n\n## Other information from RandomForest model (proximity=TRUE)\n\n# Measure of internal structure (Proximity measure)\n    # - The fraction of trees in which each plot falls in the same terminal node.\n\n# - Similarity measure - in theory, similar data points will end up in the same terminal node.\n\n\n## Let's try adding proximity to CARBON_KG model\nset.seed(66)\ncarb.mod <- randomForest(CARBON_KG ~ TMB5 + TMndvi + elevm + slp + aspval,  data=modeldat, importance = TRUE, proximity = TRUE, na.action = na.exclude)\n\nnames(carb.mod)\n\ncarb.prox <- carb.mod$proximity\n\n#you can now explore carb.prox\nThe final step in the analysis is to apply the model to the full dataset, in order to map the predicted outcomes.\n\n3.1.1 Resources this week\n\nTutorial - Linear Regression\nPredictive analytics post"
  },
  {
    "objectID": "dataanalytic-critique.html#parameterizationvalidation-iii",
    "href": "dataanalytic-critique.html#parameterizationvalidation-iii",
    "title": "3  Data/Analytic Critique",
    "section": "3.2 Parameterization/Validation I/II",
    "text": "3.2 Parameterization/Validation I/II\nDemo in class"
  },
  {
    "objectID": "dataanalytic-critique.html#modelling-socio-technical-critique",
    "href": "dataanalytic-critique.html#modelling-socio-technical-critique",
    "title": "3  Data/Analytic Critique",
    "section": "3.3 Modelling: Socio-Technical Critique",
    "text": "3.3 Modelling: Socio-Technical Critique\n\n3.3.1 Resources this week\n\nBrundson and Comber 2021\nBlair et al. 2019"
  },
  {
    "objectID": "data-to-decisions.html#communications-strategies-for-eda",
    "href": "data-to-decisions.html#communications-strategies-for-eda",
    "title": "4  Data to Decisions",
    "section": "4.1 Communications strategies for EDA",
    "text": "4.1 Communications strategies for EDA\nCommunicating with data is ultimately about telling stories, even when employing the most complex data processing pipelines and computational models. Ultimately, the outptus generated from environmental data analytics are used to improve understanding and decision making on real-world issues. As such, we consider how we tailor our message to specific audiences carefully. As with all knowledge mobilization, we need to consider a range of tools available for communicating insights gained from data intensive analysis, including but not limited to:\n\npublished papers\nblog posts\nconference presentations\ntalks at community events\nworkshops (academic, community, stakeholder), technical or domain-driven\nsoftware and data documentation\nsocial media posts (e.g., twitter)\n\n\n4.1.1 Resources this week\nThere is no hard reciple for what to say in a given communication medium. Most advice out there pertain to research talks in a conference style presentation format, and you can build your own synthesis of these guides to help you structure what to include and what to exclude. We will focus mostly on this reading - which can be adapted to a variety of contexts\n\nFarrel - short paper\n\nThough other helpful tips are available in the following sources (and many others)\n\nJones et al. - short paper\n\nvideo\n\nWeirich - powerpoint\nFleming - Nature article"
  },
  {
    "objectID": "assignments.html#assignment-1---exploratory-visualization-in-ggplot2",
    "href": "assignments.html#assignment-1---exploratory-visualization-in-ggplot2",
    "title": "5  Assignments",
    "section": "5.1 Assignment 1 - Exploratory Visualization in ggplot2",
    "text": "5.1 Assignment 1 - Exploratory Visualization in ggplot2\nYour objective in this assignment is to select a dataset and do some exploratory analysis and visualization. You should select a dataset that interests you from this online repository of datasets from various R packages. The dataset you select should be different from other members of the class. Questions 1-4 should be completed with on selected dataset and question 5 should be completed with another dataset.\nYou have to structure your assignment as a RMarkdown file. Watch this video (~45 mins) to learn how to start and navigate markdown files in R-Studio.\nYou can use the following as a template [download to your working directory, and open in R-studio to edit/knit] to get you started for your assignment.\nYour assignment should answer the following questions and be totally reproducible. This means that you must read data in from a URL so that I can replicate your work, do not include any external data files in your submission, only submit one Rmd file.\n\n5.1.1 Questions\n\nExplain in a couple of sentences why you selected this dataset. [2 marks]\nCreate a bar plot using ggplot2. Write a sentence interpreting what is happening in the graph. Make sure you read up on the documentation of the data to understand what is represented and how it was measured. [5 marks]\nCreate a line or point plot using ggplot2. Write a sentence interpreting what is happening in the graph. Make sure you read up on the documentation of the data to understand what is represented and how it was measured. [5 marks]\nCreate any other plot of your choosing using ggplot2. Write a sentence interpreting what is happening in the graph. Make sure you read up on the documentation of the data to understand what is represented and how it was measured. [5 marks]\nSelect another dataset and tell a story with the data. Think about who a potential audience is for your story. Using the structure outlined in lecture slides, write a sentence or two for each phase of the data story: Opening, Challenge, Action, and Resolution. For each, include a visualization to illustrate (which may or may not be a graph of the data- external sources can be used here as well). In order to understand the context of the data, you will have to do some background research into the dataset this is generally available in the documentation. The story should include at least three graphs you create from the data(which must be labelled correctly and not be duplicates of graphs for questions 2-4). [15 marks]\nWhat might or could you have done differently in question 5 if you were preparing the analysis for another audience/user community? [5 marks]\n\n\n\n5.1.2 Submission\nSubmit via email to Jason at the start of class on February 1."
  },
  {
    "objectID": "assignments.html#assignment-2---spatial-and-temporal-visualization-of-urban-data",
    "href": "assignments.html#assignment-2---spatial-and-temporal-visualization-of-urban-data",
    "title": "5  Assignments",
    "section": "5.2 Assignment 2 - Spatial and temporal visualization of urban data",
    "text": "5.2 Assignment 2 - Spatial and temporal visualization of urban data\nYour objective in this assignment is to perform a real-world analysis of geospatial data from an urban centre. In this assignment you are taking on the role of an geospatial consultant tasked with analyzing aspects of the transportation system in a Canadian city that will be assigned to you. The specific exploration is up to you and to some extent will be a function of the available data, but feel free to do some background research.\n\nA - Montreal\nB - Vancouver\nC - Toronto\nD - Calgary\n\nTransportation issues are always top of mind in Canadian cities and you can choose to focus on aspects of the street network, availability of transit relative to population, actual commuting data, etc. It is up to you to define a reasonable question to explore and to source appropriate data for this assignment. Every city has an open data portal with some data that will likely be of help, however you can also seek data from other sources. I will be happy to help you with accessing datasets.\nThe outcome of this project is an Rmarkdown or Quarto report report detailing the issues explored (approximately 5 written pages double-spaced), the datasets, and the analysis performed. Visualization outputs should be presented in an appendix and an accompanying 10 minute presentation to be done in class on March 8th (we might do these via Zoom).\nThe assignment will be graded on the following criteria:\n\ndepth of analysis (25%)\nvisual aesthetics (25%)\nreproducibility (10%)\nanalysis interpretation and accuracy (20%)\npresentation (20%)\n\nRecall some of the ideas about story-telling with data as you think through and put together your analysis and report.\n\n5.2.1 Submission\nSubmit the report (Rmarkdown or Quarto file plus PDF file) via email to Jason at the start of class on March 8reproducibility. Presentations will take place that day as well."
  },
  {
    "objectID": "assignments.html#assignment-3---tidy-tuesday-submission",
    "href": "assignments.html#assignment-3---tidy-tuesday-submission",
    "title": "5  Assignments",
    "section": "5.3 Assignment 3 - Tidy Tuesday Submission",
    "text": "5.3 Assignment 3 - Tidy Tuesday Submission\nFor this assignment we will complete a Tidy Tuesday submission. Tidy Tuesday came out of the R4DS (R for data science) Texbook and Online Learning Community by Hadley Wickham and Garrett Grolemund. A second edition is currently being written. Every week a new data set (i.e., data are partly preprocessed but not completely tidy) alongside an article or a source is posted on the Tidy Tuesday GitHub repository.\nThe goal is to explore this data set using data wrangling and summarizing techniques and (a lot of) insightful visualizations. You will only have around 24 hours to create a visualization based on these data. If you use any external sources (e.g., other submissions) for ideas or as sources you must cite them, though you should strive to create a totally original visualization. You will need to submit code (R script) used to produce the visualization as well as an image file (JPG, PNG, or PDF) of the submission. The data are released on GitHub on Monday and your submission must be handed in by end of day Tuesday, March 22.\nParticipation in the Tidy Tuesday project may also serve to be an introduction to the wider R user community. There is a very active R user community on Twitter, people livestream their R coding sessions on YouTube, and Tidy Tuesday submissions are typically shared on Twitter with the #TidyTuesday hashtag. You are welcome to share your submission with the wider community after you submit it."
  },
  {
    "objectID": "assignments.html#term-project",
    "href": "assignments.html#term-project",
    "title": "5  Assignments",
    "section": "5.4 Term Project",
    "text": "5.4 Term Project\nYour task for a term project is to complete an independent data analysis project on a topic of your choosing. You must source your own dataset for this project. The project should address some clearly articulated questions and include enough contextual information about the problem in order to motivate your analysis.\nYou will create two outputs from your work:\n\na written report and\na class presentations (~15 minutes) summarizing your work.\n\nYour report should follow the following structure:\n\nIntroduction (3-5 pages) - outline the problem being explored with sufficient references and description of previous work. The end of the introduction section should highlight the objectives/questions being explored in the report.\nData sources (2-3 pages) - describe the data sources used for the analysis, how they were collected, sources of bias and/or data quality issues, etc. and any additional information that might impact how these data may or may not impact conclusions made from analysis.\nData wrangling (2-3 pages) - describe in general the methodology taken to acquire, clean, and prepare the data for analysis.\nData analysis and visualization (4-5 pages) - this section should describe the analyses undertaken to address the questions being explored.\n\nSpatial - your project should have an explicit spatial component - exploring spatial variation in your dataset. This may be maps but can also be spatial variables examined in novel ways.\nTemporal - your project should have an explicit temporal component - exploring temporal variation in your dataset.\n\nInterpretation (2-3 pages) - interpret your analyses and visualization that are described in the above sections. This is your results section and should focus on understanding how your analyses adds understanding or new information to the questions outlined in the introduction.\nDiscussion (2-3 pages) - discuss limitations in your analysis, links to other papers or research that has been done, provide some ideas for where further work could be done to improve on the analysis completed in your report.\nThe length of the report should be between 4000 and 6000 words.\n\nYour presentation will be done the last class, and should be no more than 12 minutes long. The structure of the presentation should mirror your report. The presentation file does not have to be submitted."
  }
]